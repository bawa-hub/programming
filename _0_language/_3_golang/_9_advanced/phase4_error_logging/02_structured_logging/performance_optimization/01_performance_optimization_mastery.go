// âš¡ PERFORMANCE OPTIMIZATION MASTERY
// Advanced performance optimization techniques for high-throughput logging
package main

import (
	"fmt"
	"io"
	"os"
	"sync"
	"sync/atomic"
	"time"
)

// ============================================================================
// ZERO-ALLOCATION LOGGING
// ============================================================================

type ZeroAllocLogger struct {
	buffer    []byte
	mu        sync.Mutex
	output    io.Writer
	formatter func([]byte, string, map[string]interface{}) []byte
}

func NewZeroAllocLogger(output io.Writer) *ZeroAllocLogger {
	return &ZeroAllocLogger{
		buffer: make([]byte, 0, 4096), // Pre-allocated buffer
		output: output,
		formatter: func(buf []byte, message string, fields map[string]interface{}) []byte {
			// Reset buffer
			buf = buf[:0]
			
			// Add timestamp
			buf = append(buf, '[')
			buf = time.Now().AppendFormat(buf, time.RFC3339)
			buf = append(buf, ']', ' ')
			
			// Add message
			buf = append(buf, message...)
			
			// Add fields if any
			if len(fields) > 0 {
				buf = append(buf, ' ', '{')
				first := true
				for k, v := range fields {
					if !first {
						buf = append(buf, ',', ' ')
					}
					first = false
					buf = append(buf, '"')
					buf = append(buf, k...)
					buf = append(buf, '"', ':')
					buf = append(buf, fmt.Sprintf("%v", v)...)
				}
				buf = append(buf, '}')
			}
			
			buf = append(buf, '\n')
			return buf
		},
	}
}

func (zal *ZeroAllocLogger) Log(message string, fields map[string]interface{}) {
	zal.mu.Lock()
	defer zal.mu.Unlock()
	
	// Format message without allocations
	zal.buffer = zal.formatter(zal.buffer, message, fields)
	
	// Write to output
	zal.output.Write(zal.buffer)
}

// ============================================================================
// OBJECT POOL
// ============================================================================

type LogEntryPool struct {
	pool sync.Pool
}

type LogEntry struct {
	Timestamp time.Time
	Level     string
	Message   string
	Fields    map[string]interface{}
}

func NewLogEntryPool() *LogEntryPool {
	return &LogEntryPool{
		pool: sync.Pool{
			New: func() interface{} {
				return &LogEntry{
					Fields: make(map[string]interface{}),
				}
			},
		},
	}
}

func (lep *LogEntryPool) Get() *LogEntry {
	entry := lep.pool.Get().(*LogEntry)
	entry.Timestamp = time.Time{}
	entry.Level = ""
	entry.Message = ""
	// Clear fields map
	for k := range entry.Fields {
		delete(entry.Fields, k)
	}
	return entry
}

func (lep *LogEntryPool) Put(entry *LogEntry) {
	lep.pool.Put(entry)
}

// ============================================================================
// LOCK-FREE RING BUFFER
// ============================================================================

type LockFreeRingBuffer struct {
	buffer    []interface{}
	head      uint64
	tail      uint64
	mask      uint64
	capacity  uint64
}

func NewLockFreeRingBuffer(capacity uint64) *LockFreeRingBuffer {
	// Ensure capacity is power of 2
	actualCapacity := uint64(1)
	for actualCapacity < capacity {
		actualCapacity <<= 1
	}
	
	return &LockFreeRingBuffer{
		buffer:   make([]interface{}, actualCapacity),
		head:     0,
		tail:     0,
		mask:     actualCapacity - 1,
		capacity: actualCapacity,
	}
}

func (rb *LockFreeRingBuffer) Push(item interface{}) bool {
	tail := atomic.LoadUint64(&rb.tail)
	head := atomic.LoadUint64(&rb.head)
	
	// Check if buffer is full
	if (tail+1)&rb.mask == head&rb.mask {
		return false
	}
	
	// Store item
	rb.buffer[tail&rb.mask] = item
	
	// Update tail
	atomic.StoreUint64(&rb.tail, tail+1)
	
	return true
}

func (rb *LockFreeRingBuffer) Pop() (interface{}, bool) {
	head := atomic.LoadUint64(&rb.head)
	tail := atomic.LoadUint64(&rb.tail)
	
	// Check if buffer is empty
	if head == tail {
		return nil, false
	}
	
	// Get item
	item := rb.buffer[head&rb.mask]
	
	// Update head
	atomic.StoreUint64(&rb.head, head+1)
	
	return item, true
}

func (rb *LockFreeRingBuffer) Size() uint64 {
	tail := atomic.LoadUint64(&rb.tail)
	head := atomic.LoadUint64(&rb.head)
	return tail - head
}

// ============================================================================
// HIGH-PERFORMANCE WORKER POOL
// ============================================================================

type WorkerPool struct {
	workers    int
	jobQueue   chan interface{}
	resultQueue chan interface{}
	done       chan struct{}
	wg         sync.WaitGroup
	processor  func(interface{}) interface{}
}

func NewWorkerPool(workers int, queueSize int, processor func(interface{}) interface{}) *WorkerPool {
	return &WorkerPool{
		workers:     workers,
		jobQueue:    make(chan interface{}, queueSize),
		resultQueue: make(chan interface{}, queueSize),
		done:        make(chan struct{}),
		processor:   processor,
	}
}

func (wp *WorkerPool) Start() {
	for i := 0; i < wp.workers; i++ {
		wp.wg.Add(1)
		go wp.worker()
	}
}

func (wp *WorkerPool) Stop() {
	close(wp.done)
	wp.wg.Wait()
	close(wp.jobQueue)
	close(wp.resultQueue)
}

func (wp *WorkerPool) worker() {
	defer wp.wg.Done()
	
	for {
		select {
		case job := <-wp.jobQueue:
			result := wp.processor(job)
			select {
			case wp.resultQueue <- result:
			case <-wp.done:
				return
			}
		case <-wp.done:
			return
		}
	}
}

func (wp *WorkerPool) Submit(job interface{}) bool {
	select {
	case wp.jobQueue <- job:
		return true
	default:
		return false
	}
}

func (wp *WorkerPool) GetResult() (interface{}, bool) {
	select {
	case result := <-wp.resultQueue:
		return result, true
	default:
		return nil, false
	}
}

// ============================================================================
// BATCH PROCESSOR
// ============================================================================

type BatchProcessor struct {
	batchSize    int
	flushInterval time.Duration
	buffer       []interface{}
	mu           sync.Mutex
	processor    func([]interface{})
	flushTimer   *time.Timer
	done         chan struct{}
}

func NewBatchProcessor(batchSize int, flushInterval time.Duration, processor func([]interface{})) *BatchProcessor {
	bp := &BatchProcessor{
		batchSize:    batchSize,
		flushInterval: flushInterval,
		buffer:       make([]interface{}, 0, batchSize),
		processor:    processor,
		done:         make(chan struct{}),
	}
	
	// Start flush timer
	bp.flushTimer = time.AfterFunc(flushInterval, bp.flush)
	
	return bp
}

func (bp *BatchProcessor) Add(item interface{}) {
	bp.mu.Lock()
	bp.buffer = append(bp.buffer, item)
	shouldFlush := len(bp.buffer) >= bp.batchSize
	bp.mu.Unlock()
	
	// Check if batch is full
	if shouldFlush {
		bp.flush()
	}
}

func (bp *BatchProcessor) flush() {
	bp.mu.Lock()
	defer bp.mu.Unlock()
	
	if len(bp.buffer) > 0 {
		// Create copy of buffer
		batch := make([]interface{}, len(bp.buffer))
		copy(batch, bp.buffer)
		
		// Reset buffer
		bp.buffer = bp.buffer[:0]
		
		// Process batch
		go bp.processor(batch)
	}
	
	// Reset timer
	bp.flushTimer.Reset(bp.flushInterval)
}

func (bp *BatchProcessor) Stop() {
	close(bp.done)
	bp.flushTimer.Stop()
	bp.flush()
}

// ============================================================================
// CACHE-OPTIMIZED DATA STRUCTURES
// ============================================================================

type CacheOptimizedMap struct {
	shards    []*shard
	shardMask uint32
}

type shard struct {
	mu   sync.RWMutex
	data map[string]interface{}
}

func NewCacheOptimizedMap(shards int) *CacheOptimizedMap {
	// Ensure shards is power of 2
	actualShards := 1
	for actualShards < shards {
		actualShards <<= 1
	}
	
	cm := &CacheOptimizedMap{
		shards:    make([]*shard, actualShards),
		shardMask: uint32(actualShards - 1),
	}
	
	for i := 0; i < actualShards; i++ {
		cm.shards[i] = &shard{
			data: make(map[string]interface{}),
		}
	}
	
	return cm
}

func (cm *CacheOptimizedMap) getShard(key string) *shard {
	hash := fnv1aHash(key)
	return cm.shards[hash&cm.shardMask]
}

func (cm *CacheOptimizedMap) Set(key string, value interface{}) {
	shard := cm.getShard(key)
	shard.mu.Lock()
	defer shard.mu.Unlock()
	shard.data[key] = value
}

func (cm *CacheOptimizedMap) Get(key string) (interface{}, bool) {
	shard := cm.getShard(key)
	shard.mu.RLock()
	defer shard.mu.RUnlock()
	value, exists := shard.data[key]
	return value, exists
}

func (cm *CacheOptimizedMap) Delete(key string) {
	shard := cm.getShard(key)
	shard.mu.Lock()
	defer shard.mu.Unlock()
	delete(shard.data, key)
}

// FNV-1a hash function
func fnv1aHash(s string) uint32 {
	hash := uint32(2166136261)
	for _, c := range s {
		hash ^= uint32(c)
		hash *= 16777619
	}
	return hash
}

// ============================================================================
// MEMORY POOL
// ============================================================================

type MemoryPool struct {
	pool sync.Pool
	size int
}

func NewMemoryPool(size int) *MemoryPool {
	return &MemoryPool{
		pool: sync.Pool{
			New: func() interface{} {
				return make([]byte, size)
			},
		},
		size: size,
	}
}

func (mp *MemoryPool) Get() []byte {
	return mp.pool.Get().([]byte)
}

func (mp *MemoryPool) Put(buf []byte) {
	if cap(buf) == mp.size {
		mp.pool.Put(buf[:0])
	}
}

// ============================================================================
// PERFORMANCE METRICS
// ============================================================================

type PerformanceMetrics struct {
	Operations     int64
	TotalTime      time.Duration
	AverageTime    time.Duration
	MaxTime        time.Duration
	MinTime        time.Duration
	MemoryAllocs   int64
	MemoryBytes    int64
	GCCollections  int64
}

func (pm *PerformanceMetrics) Record(operationTime time.Duration) {
	atomic.AddInt64(&pm.Operations, 1)
	atomic.AddInt64((*int64)(&pm.TotalTime), int64(operationTime))
	
	// Update min/max
	for {
		currentMax := atomic.LoadInt64((*int64)(&pm.MaxTime))
		if int64(operationTime) <= currentMax {
			break
		}
		if atomic.CompareAndSwapInt64((*int64)(&pm.MaxTime), currentMax, int64(operationTime)) {
			break
		}
	}
	
	for {
		currentMin := atomic.LoadInt64((*int64)(&pm.MinTime))
		if currentMin == 0 || int64(operationTime) >= currentMin {
			break
		}
		if atomic.CompareAndSwapInt64((*int64)(&pm.MinTime), currentMin, int64(operationTime)) {
			break
		}
	}
}

func (pm *PerformanceMetrics) GetAverageTime() time.Duration {
	ops := atomic.LoadInt64(&pm.Operations)
	if ops == 0 {
		return 0
	}
	total := atomic.LoadInt64((*int64)(&pm.TotalTime))
	return time.Duration(total / ops)
}

// ============================================================================
// DEMONSTRATION FUNCTIONS
// ============================================================================

func demonstrateZeroAllocationLogging() {
	fmt.Println("\n=== Zero-Allocation Logging ===")
	
	// Create zero-allocation logger
	logger := NewZeroAllocLogger(os.Stdout)
	
	// Measure performance
	start := time.Now()
	iterations := 10000
	
	for i := 0; i < iterations; i++ {
		logger.Log("Zero allocation log message", map[string]interface{}{
			"iteration": i,
			"timestamp": time.Now().UnixNano(),
		})
	}
	
	duration := time.Since(start)
	fmt.Printf("   ðŸ“Š %d operations in %v\n", iterations, duration)
	fmt.Printf("   ðŸ“Š %.0f operations/second\n", float64(iterations)/duration.Seconds())
}

func demonstrateObjectPooling() {
	fmt.Println("\n=== Object Pooling ===")
	
	// Create object pool
	pool := NewLogEntryPool()
	
	// Measure performance with and without pooling
	iterations := 10000
	
	// Without pooling
	start := time.Now()
	for i := 0; i < iterations; i++ {
		entry := &LogEntry{
			Timestamp: time.Now(),
			Level:     "INFO",
			Message:   "Test message",
			Fields:    map[string]interface{}{"id": i},
		}
		_ = entry
	}
	withoutPool := time.Since(start)
	
	// With pooling
	start = time.Now()
	for i := 0; i < iterations; i++ {
		entry := pool.Get()
		entry.Timestamp = time.Now()
		entry.Level = "INFO"
		entry.Message = "Test message"
		entry.Fields["id"] = i
		pool.Put(entry)
	}
	withPool := time.Since(start)
	
	fmt.Printf("   ðŸ“Š Without pooling: %v\n", withoutPool)
	fmt.Printf("   ðŸ“Š With pooling: %v\n", withPool)
	fmt.Printf("   ðŸ“Š Improvement: %.1fx\n", float64(withoutPool)/float64(withPool))
}

func demonstrateLockFreeRingBuffer() {
	fmt.Println("\n=== Lock-Free Ring Buffer ===")
	
	// Create ring buffer
	rb := NewLockFreeRingBuffer(1024)
	
	// Test concurrent operations
	var wg sync.WaitGroup
	operations := 10000
	
	// Producer
	wg.Add(1)
	go func() {
		defer wg.Done()
		for i := 0; i < operations; i++ {
			for !rb.Push(fmt.Sprintf("item-%d", i)) {
				// Buffer full, retry
				time.Sleep(time.Microsecond)
			}
		}
	}()
	
	// Consumer
	wg.Add(1)
	go func() {
		defer wg.Done()
		consumed := 0
		for consumed < operations {
			if item, ok := rb.Pop(); ok {
				_ = item
				consumed++
			} else {
				time.Sleep(time.Microsecond)
			}
		}
	}()
	
	start := time.Now()
	wg.Wait()
	duration := time.Since(start)
	
	fmt.Printf("   ðŸ“Š %d operations in %v\n", operations*2, duration)
	fmt.Printf("   ðŸ“Š %.0f operations/second\n", float64(operations*2)/duration.Seconds())
}

func demonstrateWorkerPool() {
	fmt.Println("\n=== Worker Pool ===")
	
	// Create worker pool
	processor := func(input interface{}) interface{} {
		// Simulate some work
		time.Sleep(time.Microsecond)
		return fmt.Sprintf("processed-%v", input)
	}
	
	pool := NewWorkerPool(4, 1000, processor)
	pool.Start()
	defer pool.Stop()
	
	// Submit jobs
	start := time.Now()
	jobs := 1000
	
	for i := 0; i < jobs; i++ {
		pool.Submit(fmt.Sprintf("job-%d", i))
	}
	
	// Collect results
	results := 0
	for results < jobs {
		if _, ok := pool.GetResult(); ok {
			results++
		}
	}
	
	duration := time.Since(start)
	fmt.Printf("   ðŸ“Š %d jobs processed in %v\n", jobs, duration)
	fmt.Printf("   ðŸ“Š %.0f jobs/second\n", float64(jobs)/duration.Seconds())
}

func demonstrateBatchProcessing() {
	fmt.Println("\n=== Batch Processing ===")
	
	// Create batch processor
	processor := func(batch []interface{}) {
		// Simulate batch processing
		time.Sleep(time.Millisecond)
	}
	
	bp := NewBatchProcessor(100, 10*time.Millisecond, processor)
	defer bp.Stop()
	
	// Add items
	start := time.Now()
	items := 1000
	
	for i := 0; i < items; i++ {
		bp.Add(fmt.Sprintf("item-%d", i))
	}
	
	// Wait for processing
	time.Sleep(100 * time.Millisecond)
	duration := time.Since(start)
	
	fmt.Printf("   ðŸ“Š %d items batched in %v\n", items, duration)
	fmt.Printf("   ðŸ“Š %.0f items/second\n", float64(items)/duration.Seconds())
}

func demonstrateCacheOptimization() {
	fmt.Println("\n=== Cache Optimization ===")
	
	// Create cache-optimized map
	cm := NewCacheOptimizedMap(16)
	
	// Test concurrent operations
	var wg sync.WaitGroup
	operations := 10000
	
	// Writers
	for i := 0; i < 4; i++ {
		wg.Add(1)
		go func(worker int) {
			defer wg.Done()
			for j := 0; j < operations/4; j++ {
				key := fmt.Sprintf("key-%d-%d", worker, j)
				cm.Set(key, fmt.Sprintf("value-%d-%d", worker, j))
			}
		}(i)
	}
	
	// Readers
	for i := 0; i < 4; i++ {
		wg.Add(1)
		go func(worker int) {
			defer wg.Done()
			for j := 0; j < operations/4; j++ {
				key := fmt.Sprintf("key-%d-%d", worker, j)
				cm.Get(key)
			}
		}(i)
	}
	
	start := time.Now()
	wg.Wait()
	duration := time.Since(start)
	
	fmt.Printf("   ðŸ“Š %d operations in %v\n", operations*2, duration)
	fmt.Printf("   ðŸ“Š %.0f operations/second\n", float64(operations*2)/duration.Seconds())
}

func demonstrateMemoryOptimization() {
	fmt.Println("\n=== Memory Optimization ===")
	
	// Create memory pool
	pool := NewMemoryPool(1024)
	
	// Test memory allocation patterns
	iterations := 10000
	
	// Without pooling
	start := time.Now()
	for i := 0; i < iterations; i++ {
		buf := make([]byte, 1024)
		_ = buf
	}
	withoutPool := time.Since(start)
	
	// With pooling
	start = time.Now()
	for i := 0; i < iterations; i++ {
		buf := pool.Get()
		_ = buf
		pool.Put(buf)
	}
	withPool := time.Since(start)
	
	fmt.Printf("   ðŸ“Š Without pooling: %v\n", withoutPool)
	fmt.Printf("   ðŸ“Š With pooling: %v\n", withPool)
	fmt.Printf("   ðŸ“Š Improvement: %.1fx\n", float64(withoutPool)/float64(withPool))
}

func demonstratePerformanceProfiling() {
	fmt.Println("\n=== Performance Profiling ===")
	
	// Create performance metrics
	metrics := &PerformanceMetrics{}
	
	// Simulate operations
	iterations := 10000
	for i := 0; i < iterations; i++ {
		start := time.Now()
		
		// Simulate some work
		_ = fmt.Sprintf("operation-%d", i)
		
		operationTime := time.Since(start)
		metrics.Record(operationTime)
	}
	
	fmt.Printf("   ðŸ“Š Operations: %d\n", atomic.LoadInt64(&metrics.Operations))
	fmt.Printf("   ðŸ“Š Total time: %v\n", time.Duration(atomic.LoadInt64((*int64)(&metrics.TotalTime))))
	fmt.Printf("   ðŸ“Š Average time: %v\n", metrics.GetAverageTime())
	fmt.Printf("   ðŸ“Š Max time: %v\n", time.Duration(atomic.LoadInt64((*int64)(&metrics.MaxTime))))
	fmt.Printf("   ðŸ“Š Min time: %v\n", time.Duration(atomic.LoadInt64((*int64)(&metrics.MinTime))))
}

func main() {
	fmt.Println("âš¡ PERFORMANCE OPTIMIZATION MASTERY")
	fmt.Println("===================================")
	
	demonstrateZeroAllocationLogging()
	demonstrateObjectPooling()
	demonstrateLockFreeRingBuffer()
	demonstrateWorkerPool()
	demonstrateBatchProcessing()
	demonstrateCacheOptimization()
	demonstrateMemoryOptimization()
	demonstratePerformanceProfiling()
	
	fmt.Println("\nðŸŽ‰ PERFORMANCE OPTIMIZATION MASTERY COMPLETE!")
	fmt.Println("You have successfully demonstrated:")
	fmt.Println("âœ… Zero-allocation logging patterns")
	fmt.Println("âœ… Object pooling and reuse")
	fmt.Println("âœ… Lock-free data structures")
	fmt.Println("âœ… High-performance worker pools")
	fmt.Println("âœ… Batch processing optimization")
	fmt.Println("âœ… Cache-optimized data structures")
	fmt.Println("âœ… Memory optimization techniques")
	fmt.Println("âœ… Performance profiling and metrics")
	
	fmt.Println("\nðŸš€ You are now ready for Tracing and Metrics Mastery!")
}
